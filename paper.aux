\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction and Background}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}System Modeling and Simulation Environment}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Reinforcement Learning Frramework}{4}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {4}Formal Methods Integration in the Learning Loop}{8}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {5}Comparison of Continuous RL Agents}{10}{section.5}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Selected performance metrics for each RL agent (best values in bold). PPO excels in most categories, achieving the lowest tracking errors (K2, K3), best base stability (K4), and smoothest control (K7), while maintaining short training time. TRPO is a close second in many metrics but requires more training time. Off-policy methods (TD3, SAC, DDPG) show larger errors or instabilities (e.g., high T1 for DDPG) despite some strengths (DDPGâ€™s low max error K3). (KPI data from evaluation of 30 episodes per agent.)}}{11}{table.caption.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Optimization of the PPO Agent}{14}{section.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance of PPO agent before and after optimization. The optimized PPO shows substantial improvements in all aspects (higher rewards, lower errors, zero safety violations, smoother and more efficient control).}}{16}{table.caption.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion and Conclusion}{17}{section.7}\protected@file@percent }
\bibcite{SuttonBarto2018}{1}
\bibcite{SchulmanTRPO2015}{2}
\bibcite{SchulmanPPO2017}{3}
\bibcite{LillicrapDDPG2015}{4}
\bibcite{FujimotoTD32018}{5}
\bibcite{HaarnojaSAC2018}{6}
\bibcite{NanosPapadopoulos2017}{7}
\bibcite{TobinDomainRand2017}{8}
\gdef \@abspage@last{21}
